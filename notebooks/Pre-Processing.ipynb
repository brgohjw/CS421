{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import emoji\n",
    "import string\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/raw_trump_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tweets between 20th jan 2017 and 31st Dec 2018 (as we have stock data between these two dates)\n",
    "tweets['date']= pd.to_datetime(tweets['date'])\n",
    "tweets = tweets[tweets['date'] >= np.datetime64('2017-01-20')]\n",
    "tweets = tweets[tweets['date'] <= np.datetime64('2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_remove_duplicates(tweets, date):\n",
    "    duplicated_rows = tweets[tweets['date'] == date]\n",
    "    id = max(duplicated_rows['id'])\n",
    "    max_favorite = max(duplicated_rows['favorites'])\n",
    "    max_retweet = max(duplicated_rows['retweets'])\n",
    "    combined_tweet = \"\"\n",
    "    device = duplicated_rows['device'].iloc[0]\n",
    "    for index, row in duplicated_rows.iterrows():\n",
    "        combined_tweet = row['text'] + ' ' + combined_tweet.strip('.')\n",
    "    tweets = tweets[tweets.date != date]\n",
    "    row = {'id': id, 'text': combined_tweet, 'isRetweet': 'f', 'isDeleted': 'f', 'device': device, 'favorites': max_favorite, 'retweets': max_retweet, 'date': date}\n",
    "    tweets = tweets.append(row, ignore_index = True)   \n",
    "    print(f\"Combined {len(duplicated_rows)} tweets on {date}\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 tweets on 2018-11-21T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T21:59:00.000000000\n",
      "Combined 2 tweets on 2018-08-14T01:37:00.000000000\n",
      "Combined 2 tweets on 2018-05-31T10:56:00.000000000\n",
      "Combined 2 tweets on 2018-02-01T22:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T17:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:50:00.000000000\n",
      "Combined 2 tweets on 2017-12-29T12:48:00.000000000\n",
      "Combined 3 tweets on 2017-12-28T22:17:00.000000000\n",
      "Combined 2 tweets on 2017-12-22T20:47:00.000000000\n",
      "Combined 2 tweets on 2017-12-16T03:09:00.000000000\n",
      "Combined 2 tweets on 2017-11-18T13:49:00.000000000\n",
      "Combined 2 tweets on 2017-11-15T10:22:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:53:00.000000000\n",
      "Combined 2 tweets on 2017-07-11T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-28T21:09:00.000000000\n",
      "Combined 2 tweets on 2017-10-26T01:47:00.000000000\n",
      "Combined 2 tweets on 2017-10-14T11:08:00.000000000\n",
      "Combined 2 tweets on 2017-05-10T10:31:00.000000000\n",
      "Combined 2 tweets on 2017-09-30T01:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-22T10:30:00.000000000\n",
      "Combined 2 tweets on 2017-09-20T10:00:00.000000000\n",
      "Combined 2 tweets on 2017-09-17T12:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:37:00.000000000\n",
      "Combined 2 tweets on 2017-10-09T10:27:00.000000000\n",
      "Combined 2 tweets on 2017-07-09T12:51:00.000000000\n",
      "Combined 2 tweets on 2017-08-29T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-28T13:08:00.000000000\n",
      "Combined 2 tweets on 2017-08-24T13:15:00.000000000\n",
      "Combined 2 tweets on 2017-08-19T20:41:00.000000000\n",
      "Combined 2 tweets on 2017-08-18T15:27:00.000000000\n",
      "Combined 2 tweets on 2017-08-16T11:32:00.000000000\n",
      "Combined 2 tweets on 2017-08-15T11:03:00.000000000\n",
      "Combined 2 tweets on 2017-11-08T11:12:00.000000000\n",
      "Combined 2 tweets on 2017-10-08T01:18:00.000000000\n",
      "Combined 2 tweets on 2017-09-08T11:22:00.000000000\n",
      "Combined 2 tweets on 2017-08-08T10:59:00.000000000\n",
      "Combined 3 tweets on 2017-04-08T13:18:00.000000000\n",
      "Combined 2 tweets on 2017-10-07T11:54:00.000000000\n",
      "Combined 2 tweets on 2017-06-27T11:00:00.000000000\n",
      "Combined 2 tweets on 2017-06-26T18:16:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T08:29:00.000000000\n",
      "Combined 2 tweets on 2017-06-19T20:27:00.000000000\n",
      "Combined 2 tweets on 2017-02-06T10:32:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T12:45:00.000000000\n",
      "Combined 2 tweets on 2017-05-28T11:57:00.000000000\n",
      "Combined 2 tweets on 2017-04-27T14:39:00.000000000\n",
      "Combined 3 tweets on 2017-04-27T14:37:00.000000000\n",
      "Combined 2 tweets on 2017-04-22T15:18:00.000000000\n",
      "Combined 2 tweets on 2017-03-30T22:16:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:54:00.000000000\n",
      "Combined 2 tweets on 2017-01-20T17:51:00.000000000\n",
      "Combined 2 tweets on 2018-12-27T21:04:00.000000000\n",
      "Combined 2 tweets on 2018-12-20T22:21:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T02:07:00.000000000\n",
      "Combined 2 tweets on 2018-12-19T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-12-16T20:29:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T22:18:00.000000000\n",
      "Combined 2 tweets on 2018-12-14T18:17:00.000000000\n",
      "Combined 2 tweets on 2018-12-13T17:34:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T18:58:00.000000000\n",
      "Combined 2 tweets on 2018-08-12T14:19:00.000000000\n",
      "Combined 2 tweets on 2018-07-12T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-04-12T22:56:00.000000000\n",
      "Combined 2 tweets on 2018-01-12T15:21:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T22:14:00.000000000\n",
      "Combined 2 tweets on 2018-11-29T16:34:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:41:00.000000000\n",
      "Combined 2 tweets on 2018-11-28T01:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T19:05:00.000000000\n",
      "Combined 2 tweets on 2018-11-27T04:40:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T20:20:00.000000000\n",
      "Combined 2 tweets on 2018-11-26T19:47:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T16:42:00.000000000\n",
      "Combined 2 tweets on 2018-11-17T00:43:00.000000000\n",
      "Combined 2 tweets on 2018-07-11T19:44:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:26:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:25:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:23:00.000000000\n",
      "Combined 2 tweets on 2018-06-11T16:20:00.000000000\n",
      "Combined 3 tweets on 2018-06-11T16:19:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:41:00.000000000\n",
      "Combined 3 tweets on 2018-04-11T15:40:00.000000000\n",
      "Combined 4 tweets on 2018-04-11T15:39:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:32:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:28:00.000000000\n",
      "Combined 2 tweets on 2018-04-11T15:27:00.000000000\n",
      "Combined 2 tweets on 2018-03-11T21:12:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T18:37:00.000000000\n",
      "Combined 2 tweets on 2018-01-11T03:34:00.000000000\n",
      "Combined 2 tweets on 2018-10-30T17:37:00.000000000\n",
      "Combined 2 tweets on 2018-10-27T21:41:00.000000000\n",
      "Combined 2 tweets on 2018-10-25T18:47:00.000000000\n",
      "Combined 2 tweets on 2018-10-22T19:18:00.000000000\n",
      "Combined 2 tweets on 2018-10-18T15:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-17T17:11:00.000000000\n",
      "Combined 2 tweets on 2018-10-16T18:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:50:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T22:33:00.000000000\n",
      "Combined 4 tweets on 2018-10-10T12:48:00.000000000\n",
      "Combined 2 tweets on 2018-10-10T12:47:00.000000000\n",
      "Combined 2 tweets on 2018-09-10T16:00:00.000000000\n",
      "Combined 2 tweets on 2018-05-10T21:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-26T19:23:00.000000000\n",
      "Combined 2 tweets on 2018-09-22T03:09:00.000000000\n",
      "Combined 2 tweets on 2018-09-21T20:24:00.000000000\n",
      "Combined 2 tweets on 2018-09-20T18:10:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:33:00.000000000\n",
      "Combined 2 tweets on 2018-09-18T15:50:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:47:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:46:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:45:00.000000000\n",
      "Combined 3 tweets on 2018-09-17T09:44:00.000000000\n",
      "Combined 2 tweets on 2018-09-17T09:43:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T13:29:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:35:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:32:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T11:31:00.000000000\n",
      "Combined 3 tweets on 2018-09-14T11:28:00.000000000\n",
      "Combined 2 tweets on 2018-09-14T00:11:00.000000000\n",
      "Combined 2 tweets on 2018-09-13T12:16:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:08:00.000000000\n",
      "Combined 2 tweets on 2018-10-09T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-08-09T01:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-09T20:41:00.000000000\n",
      "Combined 2 tweets on 2018-03-09T11:05:00.000000000\n",
      "Combined 2 tweets on 2018-02-09T01:23:00.000000000\n",
      "Combined 4 tweets on 2018-08-29T21:23:00.000000000\n",
      "Combined 2 tweets on 2018-08-28T15:02:00.000000000\n",
      "Combined 2 tweets on 2018-08-27T22:07:00.000000000\n",
      "Combined 2 tweets on 2018-08-26T13:21:00.000000000\n",
      "Combined 3 tweets on 2018-08-24T17:36:00.000000000\n",
      "Combined 2 tweets on 2018-08-23T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-08-17T19:25:00.000000000\n",
      "Combined 2 tweets on 2018-08-13T17:14:00.000000000\n",
      "Combined 2 tweets on 2018-05-08T01:43:00.000000000\n",
      "Combined 4 tweets on 2018-04-08T03:05:00.000000000\n",
      "Combined 2 tweets on 2018-07-31T17:33:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:29:00.000000000\n",
      "Combined 2 tweets on 2018-07-30T22:28:00.000000000\n",
      "Combined 4 tweets on 2018-07-29T19:09:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:34:00.000000000\n",
      "Combined 2 tweets on 2018-07-29T11:59:00.000000000\n",
      "Combined 2 tweets on 2018-07-27T15:53:00.000000000\n",
      "Combined 2 tweets on 2018-07-26T22:48:00.000000000\n",
      "Combined 2 tweets on 2018-07-25T23:42:00.000000000\n",
      "Combined 3 tweets on 2018-07-15T16:18:00.000000000\n",
      "Combined 2 tweets on 2018-07-24T11:01:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:39:00.000000000\n",
      "Combined 2 tweets on 2018-07-20T14:35:00.000000000\n",
      "Combined 2 tweets on 2018-07-15T13:40:00.000000000\n",
      "Combined 2 tweets on 2018-07-16T04:28:00.000000000\n",
      "Combined 6 tweets on 2018-07-15T13:33:00.000000000\n",
      "Combined 2 tweets on 2018-12-07T15:00:00.000000000\n",
      "Combined 3 tweets on 2018-11-07T21:11:00.000000000\n",
      "Combined 4 tweets on 2018-11-07T21:10:00.000000000\n",
      "Combined 2 tweets on 2018-11-07T12:40:00.000000000\n",
      "Combined 2 tweets on 2018-10-07T09:00:00.000000000\n",
      "Combined 4 tweets on 2018-07-07T12:06:00.000000000\n",
      "Combined 2 tweets on 2018-05-07T19:37:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:52:00.000000000\n",
      "Combined 2 tweets on 2018-03-07T23:13:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:59:00.000000000\n",
      "Combined 2 tweets on 2018-06-30T10:49:00.000000000\n",
      "Combined 2 tweets on 2018-06-28T03:24:00.000000000\n",
      "Combined 2 tweets on 2018-06-25T16:58:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T14:00:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:21:00.000000000\n",
      "Combined 3 tweets on 2018-06-23T11:19:00.000000000\n",
      "Combined 2 tweets on 2018-06-23T11:16:00.000000000\n",
      "Combined 2 tweets on 2018-06-21T20:46:00.000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 4 tweets on 2018-06-19T13:52:00.000000000\n",
      "Combined 2 tweets on 2018-06-18T13:50:00.000000000\n",
      "Combined 3 tweets on 2018-06-17T14:36:00.000000000\n",
      "Combined 2 tweets on 2018-06-15T11:56:00.000000000\n",
      "Combined 4 tweets on 2018-06-15T11:55:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T08:53:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:09:00.000000000\n",
      "Combined 2 tweets on 2018-06-14T15:08:00.000000000\n",
      "Combined 2 tweets on 2018-06-13T09:40:00.000000000\n",
      "Combined 2 tweets on 2018-12-06T20:40:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:58:00.000000000\n",
      "Combined 2 tweets on 2018-09-06T20:56:00.000000000\n",
      "Combined 2 tweets on 2018-06-06T00:40:00.000000000\n",
      "Combined 2 tweets on 2018-05-30T12:47:00.000000000\n",
      "Combined 4 tweets on 2018-05-25T12:04:00.000000000\n",
      "Combined 2 tweets on 2018-05-23T01:13:00.000000000\n",
      "Combined 2 tweets on 2018-05-16T18:40:00.000000000\n",
      "Combined 3 tweets on 2018-05-16T13:09:00.000000000\n",
      "Combined 2 tweets on 2018-08-05T20:33:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T13:10:00.000000000\n",
      "Combined 3 tweets on 2018-04-21T12:17:00.000000000\n",
      "Combined 2 tweets on 2018-04-20T10:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-19T22:30:00.000000000\n",
      "Combined 2 tweets on 2018-04-18T12:05:00.000000000\n",
      "Combined 3 tweets on 2018-04-18T09:42:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:55:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T21:34:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:26:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T17:25:00.000000000\n",
      "Combined 4 tweets on 2018-04-17T12:24:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:57:00.000000000\n",
      "Combined 2 tweets on 2018-04-17T00:56:00.000000000\n",
      "Combined 2 tweets on 2018-03-28T21:31:00.000000000\n",
      "Combined 2 tweets on 2018-03-15T17:47:00.000000000\n",
      "Combined 2 tweets on 2018-03-02T14:40:00.000000000\n",
      "Combined 2 tweets on 2018-01-28T13:18:00.000000000\n",
      "Combined 2 tweets on 2018-01-21T02:31:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:53:00.000000000\n",
      "Combined 2 tweets on 2018-01-16T00:52:00.000000000\n",
      "Combined 2 tweets on 2018-01-13T13:14:00.000000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As twitter has a 260 character limit, some tweets are split up and posted at the same time. \n",
    "We will combine such tweets.\n",
    "'''\n",
    "dates = tweets[\"date\"]\n",
    "duplicated = tweets[dates.isin(dates[dates.duplicated()])]\n",
    "duplicated_dates = duplicated['date'].unique()\n",
    "\n",
    "for date in duplicated_dates:\n",
    "    tweets = process_and_remove_duplicates(tweets, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Retweets\n",
    "tweets1 = tweets[tweets['isRetweet'] == 'f'].reset_index(drop=True)\n",
    "# Remove retweets which are not labelled as retweets \n",
    "tweets1 = tweets1[tweets1['text'].str.contains('RT @') == False]\n",
    "\n",
    "# Feauture extraction\n",
    "tweets2 = tweets1.loc[:, ['id', 'text', 'favorites', 'retweets', 'date']]\n",
    "tweets2['date_new'] = [i + datetime.timedelta(hours = 8) for i in tweets2['date']]\n",
    "tweets2['date_part'] = [i.date() for i in tweets2['date_new']]\n",
    "tweets2['time_part'] = [i.time() for i in tweets2['date_new']]\n",
    "tweets2['hour'] = [int(str(i).split(\":\")[0]) for i in tweets2['time_part']]\n",
    "tweets2['year'] = [int(str(i).split(\"-\")[0]) for i in tweets2['date_part']]\n",
    "tweets2['month'] = [int(str(i).split(\"-\")[1]) for i in tweets2['date_part']]\n",
    "tweets2 = tweets2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to clean tweets according to the requirements of different models\n",
    "\n",
    "def tweet_clean(text):\n",
    "    '''\n",
    "    Calls tweet preprocessing library to clean tweets \n",
    "    '''\n",
    "    text1 = p.clean(text)\n",
    "    return text1\n",
    "\n",
    "def tweet_clean_sentiment_analysis(text):\n",
    "    '''\n",
    "    Similar to previous function, but does not clean everything. \n",
    "    '''\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.NUMBER)\n",
    "    text1 = p.clean(text)\n",
    "    return text1\n",
    "\n",
    "def preprocess_data(data):\n",
    " #Removes Numbers\n",
    "    data = data.astype(str).str.replace(r'\\d+', '')\n",
    "    lower_text = data.str.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer =  TweetTokenizer()\n",
    "                \n",
    "    def lemmatize_text(text):\n",
    "        return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "\n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    words = lower_text.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets cleaned for topic modelling\n",
    "tweets2['text_topic_modelling'] = tweets2['text'].apply(tweet_clean)\n",
    "tweets2['text_topic_modelling'] = preprocess_data(tweets2['text_topic_modelling'])\n",
    "stop_words = stopwords.words('english')\n",
    "tweets2['text_topic_modelling'] = tweets2['text_topic_modelling'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tweets2['text_topic_modelling'] = tweets2['text_topic_modelling'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Tweets cleaned for EDA \n",
    "tweets2['text_EDA'] = tweets2['text']\n",
    "\n",
    "# Tweets cleaned for sentiment analysis\n",
    "tweets2['text_sentiment_analysis'] = tweets2['text'].apply(tweet_clean_sentiment_analysis)\n",
    "\n",
    "# Tweets cleaned for LSTM models\n",
    "tweets2['text_LSTM'] = tweets2['text'].apply(tweet_clean)\n",
    "tweets2['text_LSTM'] = preprocess_data(tweets2['text_LSTM'])\n",
    "tweets2['text_LSTM'] = tweets2['text_LSTM'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets outside trading hours \n",
    "trump_tweets = tweets2.copy()\n",
    "trump_tweets['date_new'] = pd.to_datetime(trump_tweets['date_new'])\n",
    "trump_tweets.rename(columns={\"date_new\": \"tweet_datetime\"}, inplace=True)\n",
    "trump_tweets.set_index('tweet_datetime', inplace=True, drop=False)\n",
    "trump_tweets.index.name = None\n",
    "trump_tweets = trump_tweets.between_time(datetime.time(4), datetime.time(20), include_start=True, include_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are combined after pre processing! (else kernel crashes)\n",
    "stock_price_2017 = pd.read_csv('../data/yr2017.csv')\n",
    "stock_price_2018 = pd.read_csv('../data/yr2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns \n",
    "stock_price_2017.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)\n",
    "stock_price_2018.drop(columns=['SYM_ROOT', 'SYM_SUFFIX'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to datetime objects\n",
    "stock_price_2017['TIME_M'] = pd.to_datetime(stock_price_2017['TIME_M'], format='%H:%M:%S.%f')\n",
    "stock_price_2018['TIME_M'] = pd.to_datetime(stock_price_2018['TIME_M'], format='%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seconds, milliseconds from data\n",
    "stock_price_2017['TIME_M'] = stock_price_2017['TIME_M'].dt.floor('Min').dt.time\n",
    "stock_price_2018['TIME_M'] = stock_price_2018['TIME_M'].dt.floor('Min').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minute to minute data (one entry for each minute)\n",
    "stock_price_2017.drop_duplicates(subset=['TIME_M','DATE'],inplace=True)\n",
    "stock_price_2018.drop_duplicates(subset=['TIME_M', 'DATE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv, will be used for preprocessing later!\n",
    "stock_price = pd.concat([stock_price_2017,stock_price_2018], ignore_index=True, sort=False)\n",
    "stock_price.to_csv(\"../data/stock_price.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = pd.read_csv(\"../data/stock_price.csv\", parse_dates=[['DATE', 'TIME_M']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolation of missing values\n",
    "idx = pd.date_range(stock_price.iloc[0, 0], stock_price.iloc[-1, 0], freq='1min')\n",
    "stock_price.index = pd.DatetimeIndex(stock_price.loc[:, 'DATE_TIME_M'])\n",
    "stock_price = stock_price.reindex(idx, fill_value='NaN')\n",
    "stock_price['DATE_TIME_M'] = stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_price.astype({'PRICE': 'float'})\n",
    "stock_price['PRICE'].interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_price.between_time(datetime.time(4), datetime.time(20), include_start=True, include_end=True)\n",
    "stock_price_before_merge = stock_price.copy()\n",
    "stock_price_before_merge.to_csv(\"../data/stock_price_cleaned.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Stock Data & Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets = pd.read_csv(\"../data/trump_tweets_cleaned_for_lstm.csv\")\n",
    "stock_price = pd.read_csv(\"../data/stock_price_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.rename(columns={\"DATE_TIME_M\": \"datetime\", \"PRICE\": \"price\"}, inplace=True)\n",
    "stock_price['datetime']= pd.to_datetime(stock_price['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Stock Price Difference X mins after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price_x_mins_after(trump_tweets, stock_price, window):\n",
    "    for i,x_mins in enumerate(window):\n",
    "        stock_price[f\"datetime_{x_mins}mins_before\"] = stock_price[\"datetime\"] - pd.Timedelta(f\"{x_mins} min\")\n",
    "        stock_price_merged = stock_price.merge(stock_price, how='inner', left_on=[f\"datetime_{x_mins}mins_before\"], right_on=[\"datetime\"])\n",
    "        stock_price_merged.drop(columns=[\"datetime_y\", f\"datetime_{x_mins}mins_before_y\"], inplace=True)\n",
    "        stock_price_merged.rename(columns={\"datetime_x\": f\"datetime_{x_mins}mins_after\", \"price_x\": f\"price_{x_mins}mins_after\", f\"datetime_{x_mins}mins_before_x\": \"datetime_now\", \"price_y\": \"price_now\"}, inplace=True)\n",
    "\n",
    "        stock_price_merged[f'{x_mins}mins_price_diff_abs'] = stock_price_merged[f'price_{x_mins}mins_after'] - stock_price_merged['price_now']\n",
    "        stock_price_merged[f'{x_mins}mins_price_diff_perc'] = (stock_price_merged[f'price_{x_mins}mins_after'] - stock_price_merged['price_now']) / stock_price_merged['price_now']\n",
    "\n",
    "        trump_tweets.tweet_datetime = pd.to_datetime(trump_tweets.tweet_datetime)\n",
    "        stock_price_merged.datetime_now = pd.to_datetime(stock_price_merged.datetime_now)\n",
    "        if i == 0:\n",
    "            merged = trump_tweets.merge(stock_price_merged, how='inner', left_on=['tweet_datetime'], right_on=[\"datetime_now\"])\n",
    "        else:\n",
    "            merged = merged.merge(stock_price_merged, how='inner', left_on=['tweet_datetime'], right_on=[\"datetime_now\"])\n",
    "    \n",
    "    return pd.DataFrame(merged)\n",
    "\n",
    "merged = get_stock_price_x_mins_after(trump_tweets, stock_price, [60, 45, 30, 15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'favorites', 'retweets', 'date', 'tweet_datetime', 'date_part', 'time_part', 'hour', 'year', 'month', 'text_topic_modelling', 'text_EDA', 'text_sentiment_analysis', 'text_LSTM', '60mins_price_diff_abs', '60mins_price_diff_perc', '45mins_price_diff_abs', '45mins_price_diff_perc', '30mins_price_diff_abs', '30mins_price_diff_perc', '15mins_price_diff_abs', '15mins_price_diff_perc']\n"
     ]
    }
   ],
   "source": [
    "merged = merged.drop(['datetime_15mins_after', 'price_15mins_after', 'datetime_now_y', 'datetime_30mins_before_x', 'datetime_45mins_before_x', 'datetime_60mins_before_x', 'price_now_y', 'datetime_30mins_before_y', \n",
    "                      'datetime_45mins_before_y', 'datetime_60mins_before_y', 'datetime_60mins_after', 'price_60mins_after', 'datetime_15mins_before_x_x', 'datetime_30mins_before_x_x', 'datetime_45mins_before_x_x', 'datetime_now_x', 'price_now_x', \n",
    "                      'datetime_15mins_before_y_x','datetime_30mins_before_y_x', 'datetime_45mins_before_y_x', 'datetime_45mins_after', 'price_45mins_after', 'datetime_15mins_before_x_y', 'datetime_30mins_before_x_y', \n",
    "                      'datetime_now_y', 'datetime_60mins_before_x_x', 'price_now_y', 'datetime_15mins_before_y_y', 'datetime_30mins_before_y_y', 'datetime_60mins_before_y_x', 'datetime_30mins_after', 'price_30mins_after', \n",
    "                      'datetime_15mins_before_x', 'datetime_now_x', 'datetime_45mins_before_x_y', 'datetime_60mins_before_x_y', 'price_now_x', 'datetime_15mins_before_y', 'datetime_45mins_before_y_y', 'datetime_60mins_before_y_y'], axis=1)\n",
    "\n",
    "merged.to_csv(\"../data/master_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Array of Stock Prices 30 minutes before, ie. [-30, -29, -28, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_of_prices_xmin_before(datetime_now, x_min, stock_price_merged):\n",
    "    earliest_time = (datetime_now - timedelta(minutes = x_min)).time()\n",
    "    if earliest_time >= datetime.time(4, 0):\n",
    "        result = []\n",
    "        for min_interval in range(x_min, 0, -1):\n",
    "            datetime_query = datetime_now - timedelta(minutes = min_interval)\n",
    "            price = stock_price_merged[stock_price_merged['DATE_TIME_M'] == datetime_query].iloc[0, stock_price_merged.columns.get_loc('PRICE')]\n",
    "            result.append(price)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "merged['prev_30mins_prices'] = merged['tweet_datetime'].apply(get_array_of_prices_xmin_before, args=(30, stock_price_before_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(\"../data/data_LSTM_model_b.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
